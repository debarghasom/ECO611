{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e63a5807-1f94-4ce4-b3ca-1bbe91e3b675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\t ****SiMplex Algorithm ****\n",
      "\n",
      "\n",
      "Table at itr = 0\n",
      "B \tCB \tXB \ty1 \ty2 \ty3 \ty4\n",
      "3\t0\t8\t1\t1\t0\t1\t\n",
      "2\t0\t10\t2\t1\t1\t0\t\n",
      "\n",
      "Simplex Working....\n",
      "Iteration:  1\n",
      "B \tCB \tXB \ty1 \ty2 \ty3 \ty4\n",
      "3\t0\t8\t1\t1\t0\t1\t\n",
      "2\t0\t10\t2\t1\t1\t0\t\n",
      "rel profit:  1, 1, 0, 0, \n",
      "\n",
      "pivot element index: [1 3]\n",
      "pivot element:  2\n",
      "\n",
      "\n",
      "Iteration:  2\n",
      "B \tCB \tXB \ty1 \ty2 \ty3 \ty4\n",
      "3\t0\t3\t0\t1/2\t-1/2\t1\t\n",
      "0\t1\t5\t1\t1/2\t1/2\t0\t\n",
      "rel profit:  0, 1/2, -1/2, 0, \n",
      "\n",
      "pivot element index: [0 4]\n",
      "pivot element:  1/2\n",
      "\n",
      "\n",
      "Iteration:  3\n",
      "B \tCB \tXB \ty1 \ty2 \ty3 \ty4\n",
      "1\t1\t6\t0\t1\t-1\t2\t\n",
      "0\t1\t2\t1\t0\t1\t-1\t\n",
      "rel profit:  0, 0, 0, -1, \n",
      "Case of Alternate found\n",
      "\n",
      "All profits are <= 0, optimality reached\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from fractions import Fraction # so that numbers are not displayed in decimal.\n",
    "\n",
    "print(\"\\n\t\t\t\t ****SiMplex Algorithm ****\\n\\n\")\n",
    "\n",
    "# inputs \n",
    "\n",
    "# A will contain the coefficients of the constraints\n",
    "A = np.array([[1,1,1,0], [1,0,0,1]])\n",
    "# b will contain the amount of resources \n",
    "b = np.array([2,1])\t\t \n",
    "# c will contain coefficients of objective function Z\t \n",
    "c = np.array([-3,-1,0,0])\t\t\t \n",
    "\n",
    "# B will contain the basic variables that make identity matrix\n",
    "cb = np.array(c[3])\n",
    "B = np.array([[3], [2]])\t\t \n",
    "# cb contains their corresponding coefficients in Z \n",
    "cb = np.vstack((cb, c[2]))\t \n",
    "xb = np.transpose([b])\t\t\t\t \n",
    "# combine matrices B and cb\n",
    "table = np.hstack((B, cb))\t\t\t \n",
    "table = np.hstack((table, xb))\t\t \n",
    "# combine matrices B, cb and xb\n",
    "# finally combine matrix A to form the complete simplex table\n",
    "table = np.hstack((table, A))\t\t \n",
    "# change the type of table to float\n",
    "table = np.array(table, dtype ='float') \n",
    "# inputs end\n",
    "\n",
    "# if min problem, make this var 1\n",
    "MIN = 0\n",
    "\n",
    "print(\"Table at itr = 0\")\n",
    "print(\"B \\tCB \\tXB \\ty1 \\ty2 \\ty3 \\ty4\")\n",
    "for row in table:\n",
    "\tfor el in row:\n",
    "\t\t\t\t# limit the denominator under 100\n",
    "\t\tprint(Fraction(str(el)).limit_denominator(100), end ='\\t') \n",
    "\tprint()\n",
    "print()\n",
    "print(\"Simplex Working....\")\n",
    "\n",
    "# when optimality reached it will be made 1\n",
    "reached = 0\t\n",
    "itr = 1\n",
    "unbounded = 0\n",
    "alternate = 0\n",
    "\n",
    "while reached == 0:\n",
    "\n",
    "\tprint(\"Iteration: \", end =' ')\n",
    "\tprint(itr)\n",
    "\tprint(\"B \\tCB \\tXB \\ty1 \\ty2 \\ty3 \\ty4\")\n",
    "\tfor row in table:\n",
    "\t\tfor el in row:\n",
    "\t\t\tprint(Fraction(str(el)).limit_denominator(100), end ='\\t')\n",
    "\t\tprint()\n",
    "\n",
    "\t# calculate Relative profits-> cj - zj for non-basics\n",
    "\ti = 0\n",
    "\trel_prof = []\n",
    "\twhile i<len(A[0]):\n",
    "\t\trel_prof.append(c[i] - np.sum(table[:, 1]*table[:, 3 + i]))\n",
    "\t\ti = i + 1\n",
    "\n",
    "\tprint(\"rel profit: \", end =\" \")\n",
    "\tfor profit in rel_prof:\n",
    "\t\tprint(Fraction(str(profit)).limit_denominator(100), end =\", \")\n",
    "\tprint()\n",
    "\ti = 0\n",
    "\t\n",
    "\tb_var = table[:, 0]\n",
    "\t# checking for alternate solution\n",
    "\twhile i<len(A[0]):\n",
    "\t\tj = 0\n",
    "\t\tpresent = 0\n",
    "\t\twhile j<len(b_var):\n",
    "\t\t\tif int(b_var[j]) == i:\n",
    "\t\t\t\tpresent = 1\n",
    "\t\t\t\tbreak;\n",
    "\t\t\tj+= 1\n",
    "\t\tif present == 0:\n",
    "\t\t\tif rel_prof[i] == 0:\n",
    "\t\t\t\talternate = 1\n",
    "\t\t\t\tprint(\"Case of Alternate found\")\n",
    "\t\t\t\t# print(i, end =\" \")\n",
    "\t\ti+= 1\n",
    "\tprint()\n",
    "\tflag = 0\n",
    "\tfor profit in rel_prof:\n",
    "\t\tif profit>0:\n",
    "\t\t\tflag = 1\n",
    "\t\t\tbreak\n",
    "\t\t# if all relative profits <= 0\n",
    "\tif flag == 0:\n",
    "\t\tprint(\"All profits are <= 0, optimality reached\")\n",
    "\t\treached = 1\n",
    "\t\tbreak\n",
    "\n",
    "\t# kth var will enter the basis\n",
    "\tk = rel_prof.index(max(rel_prof))\n",
    "\tmin = 99999\n",
    "\ti = 0;\n",
    "\tr = -1\n",
    "\t# min ratio test (only positive values)\n",
    "\twhile i<len(table):\n",
    "\t\tif (table[:, 2][i]>0 and table[:, 3 + k][i]>0): \n",
    "\t\t\tval = table[:, 2][i]/table[:, 3 + k][i]\n",
    "\t\t\tif val<min:\n",
    "\t\t\t\tmin = val\n",
    "\t\t\t\tr = i\t # leaving variable\n",
    "\t\ti+= 1\n",
    "\n",
    "\t\t# if no min ratio test was performed\n",
    "\tif r ==-1:\n",
    "\t\tunbounded = 1\n",
    "\t\tprint(\"Case of Unbounded\")\n",
    "\t\tbreak\n",
    "\n",
    "\tprint(\"pivot element index:\", end =' ')\n",
    "\tprint(np.array([r, 3 + k]))\n",
    "\n",
    "\tpivot = table[r][3 + k]\n",
    "\tprint(\"pivot element: \", end =\" \")\n",
    "\tprint(Fraction(pivot).limit_denominator(100))\n",
    "\t\t\n",
    "\t\t# perform row operations\n",
    "\t# divide the pivot row with the pivot element\n",
    "\ttable[r, 2:len(table[0])] = table[\n",
    "\t\t\tr, 2:len(table[0])] / pivot\n",
    "\t\t\t\n",
    "\t# do row operation on other rows\n",
    "\ti = 0\n",
    "\twhile i<len(table):\n",
    "\t\tif i != r:\n",
    "\t\t\ttable[i, 2:len(table[0])] = table[i,2:len(table[0])] - table[i][3 + k] *table[r, 2:len(table[0])]\n",
    "\t\ti += 1\n",
    "\n",
    "\t\n",
    "\t# assign the new basic variable\n",
    "\ttable[r][0] = k\n",
    "\ttable[r][1] = c[k]\n",
    "\t\n",
    "\tprint()\n",
    "\tprint()\n",
    "\titr+= 1\n",
    "\t\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea87ea30-831a-4ee5-be15-f55869880376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function linprog in module scipy.optimize._linprog:\n",
      "\n",
      "linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='highs', callback=None, options=None, x0=None, integrality=None)\n",
      "    Linear programming: minimize a linear objective function subject to linear\n",
      "    equality and inequality constraints.\n",
      "    \n",
      "    Linear programming solves problems of the following form:\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "        \\min_x \\ & c^T x \\\\\n",
      "        \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "        & A_{eq} x = b_{eq},\\\\\n",
      "        & l \\leq x \\leq u ,\n",
      "    \n",
      "    where :math:`x` is a vector of decision variables; :math:`c`,\n",
      "    :math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      "    :math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "    \n",
      "    Alternatively, that's:\n",
      "    \n",
      "        - minimize ::\n",
      "    \n",
      "            c @ x\n",
      "    \n",
      "        - such that ::\n",
      "    \n",
      "            A_ub @ x <= b_ub\n",
      "            A_eq @ x == b_eq\n",
      "            lb <= x <= ub\n",
      "    \n",
      "    Note that by default ``lb = 0`` and ``ub = None``. Other bounds can be\n",
      "    specified with ``bounds``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    c : 1-D array\n",
      "        The coefficients of the linear objective function to be minimized.\n",
      "    A_ub : 2-D array, optional\n",
      "        The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "        coefficients of a linear inequality constraint on ``x``.\n",
      "    b_ub : 1-D array, optional\n",
      "        The inequality constraint vector. Each element represents an\n",
      "        upper bound on the corresponding value of ``A_ub @ x``.\n",
      "    A_eq : 2-D array, optional\n",
      "        The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "        coefficients of a linear equality constraint on ``x``.\n",
      "    b_eq : 1-D array, optional\n",
      "        The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "        the corresponding element of ``b_eq``.\n",
      "    bounds : sequence, optional\n",
      "        A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "        the minimum and maximum values of that decision variable.\n",
      "        If a single tuple ``(min, max)`` is provided, then ``min`` and ``max``\n",
      "        will serve as bounds for all decision variables.\n",
      "        Use ``None`` to indicate that there is no bound. For instance, the\n",
      "        default bound ``(0, None)`` means that all decision variables are\n",
      "        non-negative, and the pair ``(None, None)`` means no bounds at all,\n",
      "        i.e. all variables are allowed to be any real.\n",
      "    method : str, optional\n",
      "        The algorithm used to solve the standard form problem.\n",
      "        :ref:`'highs' <optimize.linprog-highs>` (default),\n",
      "        :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "        :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "        :ref:`'interior-point' <optimize.linprog-interior-point>` (legacy),\n",
      "        :ref:`'revised simplex' <optimize.linprog-revised_simplex>` (legacy),\n",
      "        and\n",
      "        :ref:`'simplex' <optimize.linprog-simplex>` (legacy) are supported.\n",
      "        The legacy methods are deprecated and will be removed in SciPy 1.11.0.\n",
      "    callback : callable, optional\n",
      "        If a callback function is provided, it will be called at least once per\n",
      "        iteration of the algorithm. The callback function must accept a single\n",
      "        `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "    \n",
      "        x : 1-D array\n",
      "            The current solution vector.\n",
      "        fun : float\n",
      "            The current value of the objective function ``c @ x``.\n",
      "        success : bool\n",
      "            ``True`` when the algorithm has completed successfully.\n",
      "        slack : 1-D array\n",
      "            The (nominally positive) values of the slack,\n",
      "            ``b_ub - A_ub @ x``.\n",
      "        con : 1-D array\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            ``b_eq - A_eq @ x``.\n",
      "        phase : int\n",
      "            The phase of the algorithm being executed.\n",
      "        status : int\n",
      "            An integer representing the status of the algorithm.\n",
      "    \n",
      "            ``0`` : Optimization proceeding nominally.\n",
      "    \n",
      "            ``1`` : Iteration limit reached.\n",
      "    \n",
      "            ``2`` : Problem appears to be infeasible.\n",
      "    \n",
      "            ``3`` : Problem appears to be unbounded.\n",
      "    \n",
      "            ``4`` : Numerical difficulties encountered.\n",
      "    \n",
      "            nit : int\n",
      "                The current iteration number.\n",
      "            message : str\n",
      "                A string descriptor of the algorithm status.\n",
      "    \n",
      "        Callback functions are not currently supported by the HiGHS methods.\n",
      "    \n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        options:\n",
      "    \n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "            Default: see method-specific documentation.\n",
      "        disp : bool\n",
      "            Set to ``True`` to print convergence messages.\n",
      "            Default: ``False``.\n",
      "        presolve : bool\n",
      "            Set to ``False`` to disable automatic presolve.\n",
      "            Default: ``True``.\n",
      "    \n",
      "        All methods except the HiGHS solvers also accept:\n",
      "    \n",
      "        tol : float\n",
      "            A tolerance which determines when a residual is \"close enough\" to\n",
      "            zero to be considered exactly zero.\n",
      "        autoscale : bool\n",
      "            Set to ``True`` to automatically perform equilibration.\n",
      "            Consider using this option if the numerical values in the\n",
      "            constraints are separated by several orders of magnitude.\n",
      "            Default: ``False``.\n",
      "        rr : bool\n",
      "            Set to ``False`` to disable automatic redundancy removal.\n",
      "            Default: ``True``.\n",
      "        rr_method : string\n",
      "            Method used to identify and remove redundant rows from the\n",
      "            equality constraint matrix after presolve. For problems with\n",
      "            dense input, the available methods for redundancy removal are:\n",
      "    \n",
      "            \"SVD\":\n",
      "                Repeatedly performs singular value decomposition on\n",
      "                the matrix, detecting redundant rows based on nonzeros\n",
      "                in the left singular vectors that correspond with\n",
      "                zero singular values. May be fast when the matrix is\n",
      "                nearly full rank.\n",
      "            \"pivot\":\n",
      "                Uses the algorithm presented in [5]_ to identify\n",
      "                redundant rows.\n",
      "            \"ID\":\n",
      "                Uses a randomized interpolative decomposition.\n",
      "                Identifies columns of the matrix transpose not used in\n",
      "                a full-rank interpolative decomposition of the matrix.\n",
      "            None:\n",
      "                Uses \"svd\" if the matrix is nearly full rank, that is,\n",
      "                the difference between the matrix rank and the number\n",
      "                of rows is less than five. If not, uses \"pivot\". The\n",
      "                behavior of this default is subject to change without\n",
      "                prior notice.\n",
      "    \n",
      "            Default: None.\n",
      "            For problems with sparse input, this option is ignored, and the\n",
      "            pivot-based algorithm presented in [5]_ is used.\n",
      "    \n",
      "        For method-specific options, see\n",
      "        :func:`show_options('linprog') <show_options>`.\n",
      "    \n",
      "    x0 : 1-D array, optional\n",
      "        Guess values of the decision variables, which will be refined by\n",
      "        the optimization algorithm. This argument is currently used only by the\n",
      "        'revised simplex' method, and can only be used if `x0` represents a\n",
      "        basic feasible solution.\n",
      "    \n",
      "    integrality : 1-D array or int, optional\n",
      "        Indicates the type of integrality constraint on each decision variable.\n",
      "    \n",
      "        ``0`` : Continuous variable; no integrality constraint.\n",
      "    \n",
      "        ``1`` : Integer variable; decision variable must be an integer\n",
      "        within `bounds`.\n",
      "    \n",
      "        ``2`` : Semi-continuous variable; decision variable must be within\n",
      "        `bounds` or take value ``0``.\n",
      "    \n",
      "        ``3`` : Semi-integer variable; decision variable must be an integer\n",
      "        within `bounds` or take value ``0``.\n",
      "    \n",
      "        By default, all variables are continuous.\n",
      "    \n",
      "        For mixed integrality constraints, supply an array of shape `c.shape`.\n",
      "        To infer a constraint on each decision variable from shorter inputs,\n",
      "        the argument will be broadcasted to `c.shape` using `np.broadcast_to`.\n",
      "    \n",
      "        This argument is currently used only by the ``'highs'`` method and\n",
      "        ignored otherwise.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        A :class:`scipy.optimize.OptimizeResult` consisting of the fields\n",
      "        below. Note that the return types of the fields may depend on whether\n",
      "        the optimization was successful, therefore it is recommended to check\n",
      "        `OptimizeResult.status` before relying on the other fields:\n",
      "    \n",
      "        x : 1-D array\n",
      "            The values of the decision variables that minimizes the\n",
      "            objective function while satisfying the constraints.\n",
      "        fun : float\n",
      "            The optimal value of the objective function ``c @ x``.\n",
      "        slack : 1-D array\n",
      "            The (nominally positive) values of the slack variables,\n",
      "            ``b_ub - A_ub @ x``.\n",
      "        con : 1-D array\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            ``b_eq - A_eq @ x``.\n",
      "        success : bool\n",
      "            ``True`` when the algorithm succeeds in finding an optimal\n",
      "            solution.\n",
      "        status : int\n",
      "            An integer representing the exit status of the algorithm.\n",
      "    \n",
      "            ``0`` : Optimization terminated successfully.\n",
      "    \n",
      "            ``1`` : Iteration limit reached.\n",
      "    \n",
      "            ``2`` : Problem appears to be infeasible.\n",
      "    \n",
      "            ``3`` : Problem appears to be unbounded.\n",
      "    \n",
      "            ``4`` : Numerical difficulties encountered.\n",
      "    \n",
      "        nit : int\n",
      "            The total number of iterations performed in all phases.\n",
      "        message : str\n",
      "            A string descriptor of the exit status of the algorithm.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    show_options : Additional options accepted by the solvers.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter.\n",
      "    \n",
      "    `'highs-ds'` and\n",
      "    `'highs-ipm'` are interfaces to the\n",
      "    HiGHS simplex and interior-point method solvers [13]_, respectively.\n",
      "    `'highs'` (default) chooses between\n",
      "    the two automatically. These are the fastest linear\n",
      "    programming solvers in SciPy, especially for large, sparse problems;\n",
      "    which of these two is faster is problem-dependent.\n",
      "    The other solvers (`'interior-point'`, `'revised simplex'`, and\n",
      "    `'simplex'`) are legacy methods and will be removed in SciPy 1.11.0.\n",
      "    \n",
      "    Method *highs-ds* is a wrapper of the C++ high performance dual\n",
      "    revised simplex implementation (HSOL) [13]_, [14]_. Method *highs-ipm*\n",
      "    is a wrapper of a C++ implementation of an **i**\\ nterior-\\ **p**\\ oint\n",
      "    **m**\\ ethod [13]_; it features a crossover routine, so it is as accurate\n",
      "    as a simplex solver. Method *highs* chooses between the two automatically.\n",
      "    For new code involving `linprog`, we recommend explicitly choosing one of\n",
      "    these three method values.\n",
      "    \n",
      "    .. versionadded:: 1.6.0\n",
      "    \n",
      "    Method *interior-point* uses the primal-dual path following algorithm\n",
      "    as outlined in [4]_. This algorithm supports sparse constraint matrices and\n",
      "    is typically faster than the simplex methods, especially for large, sparse\n",
      "    problems. Note, however, that the solution returned may be slightly less\n",
      "    accurate than those of the simplex methods and will not, in general,\n",
      "    correspond with a vertex of the polytope defined by the constraints.\n",
      "    \n",
      "    .. versionadded:: 1.0.0\n",
      "    \n",
      "    Method *revised simplex* uses the revised simplex method as described in\n",
      "    [9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "    its inverse, is efficiently maintained and used to solve the linear systems\n",
      "    at each iteration of the algorithm.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Method *simplex* uses a traditional, full-tableau implementation of\n",
      "    Dantzig's simplex algorithm [1]_, [2]_ (*not* the\n",
      "    Nelder-Mead simplex). This algorithm is included for backwards\n",
      "    compatibility and educational purposes.\n",
      "    \n",
      "    .. versionadded:: 0.15.0\n",
      "    \n",
      "    Before applying *interior-point*, *revised simplex*, or *simplex*,\n",
      "    a presolve procedure based on [8]_ attempts\n",
      "    to identify trivial infeasibilities, trivial unboundedness, and potential\n",
      "    problem simplifications. Specifically, it checks for:\n",
      "    \n",
      "    - rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "    - columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "      variables;\n",
      "    - column singletons in ``A_eq``, representing fixed variables; and\n",
      "    - column singletons in ``A_ub``, representing simple bounds.\n",
      "    \n",
      "    If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "    and unbounded variable has negative cost) or infeasible (e.g., a row of\n",
      "    zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "    terminates with the appropriate status code. Note that presolve terminates\n",
      "    as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "    may be reported as unbounded when in reality the problem is infeasible\n",
      "    (but infeasibility has not been detected yet). Therefore, if it is\n",
      "    important to know whether the problem is actually infeasible, solve the\n",
      "    problem again with option ``presolve=False``.\n",
      "    \n",
      "    If neither infeasibility nor unboundedness are detected in a single pass\n",
      "    of the presolve, bounds are tightened where possible and fixed\n",
      "    variables are removed from the problem. Then, linearly dependent rows\n",
      "    of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "    infeasibility) to avoid numerical difficulties in the primary solve\n",
      "    routine. Note that rows that are nearly linearly dependent (within a\n",
      "    prescribed tolerance) may also be removed, which can change the optimal\n",
      "    solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "    your problem formulation and run with option ``rr=False`` or\n",
      "    ``presolve=False``.\n",
      "    \n",
      "    Several potential improvements can be made here: additional presolve\n",
      "    checks outlined in [8]_ should be implemented, the presolve routine should\n",
      "    be run multiple times (until no further simplifications can be made), and\n",
      "    more of the efficiency improvements from [5]_ should be implemented in the\n",
      "    redundancy removal routines.\n",
      "    \n",
      "    After presolve, the problem is transformed to standard form by converting\n",
      "    the (tightened) simple bounds to upper bound constraints, introducing\n",
      "    non-negative slack variables for inequality constraints, and expressing\n",
      "    unbounded variables as the difference between two non-negative variables.\n",
      "    Optionally, the problem is automatically scaled via equilibration [12]_.\n",
      "    The selected algorithm solves the standard form problem, and a\n",
      "    postprocessing routine converts the result to a solution to the original\n",
      "    problem.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "           Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "           1963\n",
      "    .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "           Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "    .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "           Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "    .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "           optimizer for linear programming: an implementation of the\n",
      "           homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "           2000. 197-232.\n",
      "    .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "           large-scale linear programming.\" Optimization Methods and Software\n",
      "           6.3 (1995): 219-227.\n",
      "    .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "           Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "           March 2004. Available 2/25/2017 at\n",
      "           https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "    .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "           Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "           http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "    .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "           programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "    .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "           programming.\" Athena Scientific 1 (1997): 997.\n",
      "    .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "            methods for large scale linear programming. HEC/Universite de\n",
      "            Geneve, 1996.\n",
      "    .. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "            Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "    .. [12] Tomlin, J. A. \"On scaling linear programming problems.\"\n",
      "            Mathematical Programming Study 4 (1975): 146-166.\n",
      "    .. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "            \"HiGHS - high performance software for linear optimization.\"\n",
      "            https://highs.dev/\n",
      "    .. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "            simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "            119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Consider the following problem:\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "        \\min_{x_0, x_1} \\ -x_0 + 4x_1 & \\\\\n",
      "        \\mbox{such that} \\ -3x_0 + x_1 & \\leq 6,\\\\\n",
      "        -x_0 - 2x_1 & \\geq -4,\\\\\n",
      "        x_1 & \\geq -3.\n",
      "    \n",
      "    The problem is not presented in the form accepted by `linprog`. This is\n",
      "    easily remedied by converting the \"greater than\" inequality\n",
      "    constraint to a \"less than\" inequality constraint by\n",
      "    multiplying both sides by a factor of :math:`-1`. Note also that the last\n",
      "    constraint is really the simple bound :math:`-3 \\leq x_1 \\leq \\infty`.\n",
      "    Finally, since there are no bounds on :math:`x_0`, we must explicitly\n",
      "    specify the bounds :math:`-\\infty \\leq x_0 \\leq \\infty`, as the\n",
      "    default is for variables to be non-negative. After collecting coeffecients\n",
      "    into arrays and tuples, the input for this problem is:\n",
      "    \n",
      "    >>> from scipy.optimize import linprog\n",
      "    >>> c = [-1, 4]\n",
      "    >>> A = [[-3, 1], [1, 2]]\n",
      "    >>> b = [6, 4]\n",
      "    >>> x0_bounds = (None, None)\n",
      "    >>> x1_bounds = (-3, None)\n",
      "    >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds])\n",
      "    >>> res.fun\n",
      "    -22.0\n",
      "    >>> res.x\n",
      "    array([10., -3.])\n",
      "    >>> res.message\n",
      "    'Optimization terminated successfully. (HiGHS Status 7: Optimal)'\n",
      "    \n",
      "    The marginals (AKA dual values / shadow prices / Lagrange multipliers)\n",
      "    and residuals (slacks) are also available.\n",
      "    \n",
      "    >>> res.ineqlin\n",
      "      residual: [ 3.900e+01  0.000e+00]\n",
      "     marginals: [-0.000e+00 -1.000e+00]\n",
      "    \n",
      "    For example, because the marginal associated with the second inequality\n",
      "    constraint is -1, we expect the optimal value of the objective function\n",
      "    to decrease by ``eps`` if we add a small amount ``eps`` to the right hand\n",
      "    side of the second inequality constraint:\n",
      "    \n",
      "    >>> eps = 0.05\n",
      "    >>> b[1] += eps\n",
      "    >>> linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]).fun\n",
      "    -22.05\n",
      "    \n",
      "    Also, because the residual on the first inequality constraint is 39, we\n",
      "    can decrease the right hand side of the first constraint by 39 without\n",
      "    affecting the optimal solution.\n",
      "    \n",
      "    >>> b = [6, 4]  # reset to original values\n",
      "    >>> b[0] -= 39\n",
      "    >>> linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]).fun\n",
      "    -22.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "help(sc.optimize.linprog)\n",
    "print(sc.optimize.linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76740a2a-8623-4c71-b911-c9ae223af338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
